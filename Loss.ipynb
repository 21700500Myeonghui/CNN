{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wingnim.tistory.com/34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross entropy example</h2>\n",
    "<body>\n",
    "    <br>\n",
    "torch.nn.CrossEntropyLoss()함수를 쓰지 않을 경우,<br>\n",
    "softmax를 하고 넘겨주어야 함.<br>\n",
    "label을 one-hot encoding으로 넘겨주어야 함.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 =  0.35667494393873245\n",
      "loss2 =  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#One hot \n",
    "#0: 1 0 0\n",
    "#1: 0 1 0\n",
    "#2: 0 0 1\n",
    "Y = np.array([1,0,0])\n",
    "\n",
    "Y_pred1 = np.array([0.7,0.2,0.1])\n",
    "Y_pred2 = np.array([0.1,0.3,0.6])\n",
    "\n",
    "print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1))) #0.35\n",
    "print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2))) #2.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Softmax + CELoss in pytorch</h2>\n",
    "<body>\n",
    "    <br>\n",
    "softmax를 하지 않고 넘겨주어도 됨.<br>\n",
    "label을 one-hot encoding으로 넘겨줄 필요 없이 그냥 class label 숫자 하나만 넘겨주면 됨.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4170) tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Y=torch.LongTensor([0])\n",
    "Y.requires_grad = False\n",
    "\n",
    "Y_pred1 = torch.Tensor([[2.0,1.0,0.1]])\n",
    "Y_pred2 = torch.Tensor([[0.5,2.0,0.3]])\n",
    "\n",
    "l1 = loss(Y_pred1,Y)\n",
    "l2 = loss(Y_pred2,Y)\n",
    "\n",
    "print(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Loss</h2>\n",
    "<body>\n",
    "    <br>\n",
    "batch 단위의 loss를 구할 수 있음<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1518) tensor(0.4812)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.LongTensor([2,0,1])\n",
    "Y.requires_grad = False\n",
    "\n",
    "Y_pred1 = torch.Tensor([[2.0,1.0,0.1],\n",
    "                      [1.0,1.4,0.1], [1.1,0.2,3.1]])\n",
    "Y_pred2 = torch.Tensor([[0.5,1.0,2.5], [2.2,1.1,0.3], [1.5,2.0,1.3]])\n",
    "\n",
    "l1 = loss(Y_pred1,Y)\n",
    "l2 = loss(Y_pred2,Y)#두번째가 정답에 가깝기 때문에 loss가 적음\n",
    "\n",
    "print(l1,l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NULL Loss</h2>\n",
    "<body>\n",
    "    <br>\n",
    "마음데로 Softmax를 취해준다는 점이 마음에 안 들 경우 사용<br>\n",
    "CELoss는 결국 Sotfmax + NULL Loss라고 볼 수 있음<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
